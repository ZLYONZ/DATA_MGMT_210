{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc11292-31b5-48f5-99be-480ecf5989e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.1\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "with open('pokemonTrain.csv','r') as pokemonfile:\n",
    "    lines = csv.reader(pokemonfile)\n",
    "    for line in lines:\n",
    "        data.append(line)\n",
    "\n",
    "cols = {}\n",
    "names = data[0]\n",
    "for i, col in enumerate(data[0]):\n",
    "    cols[col] = i   \n",
    "num_of_pokemon = len(data[1:])\n",
    "fire_type = [x for x in data[1:] if x[cols['type']] == 'fire']\n",
    "fire_type_40 = [x for x in fire_type if float(x[cols['level']]) >= 40]\n",
    "\n",
    "print(len(fire_type))\n",
    "print(len(fire_type_40))\n",
    "print('Percentage of fire type Pokemons at or above level 40 = {}%.'.format(round(len(fire_type_40)/num_of_pokemon*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a034394-f04f-4562-921b-39833218b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1.2\n",
    "def get_type_weak():\n",
    "    types = set([x[cols['type']] for x in data[1:]])\n",
    "    weakness = set([x[cols['weakness']] for x in data[1:]])\n",
    "    weak_dict = {}\n",
    "    \n",
    "    for typ in types:\n",
    "        tmp_typ = [x for x in data[1:] if x[cols['type']] == typ]\n",
    "        weak = []\n",
    "        for wk in weakness:\n",
    "            tmp_wk = [x for x in tmp_typ if x[cols['weakness']] == wk]\n",
    "            weak.append((wk, len(tmp_wk)))\n",
    "        weak.sort(key=lambda x: (-x[-1], x[0])) \n",
    "        weak_dict[typ] = weak[0][0]\n",
    "\n",
    "    return weak_dict\n",
    "\n",
    "weak_dict = get_type_weak()\n",
    "\n",
    "def wk2tp(wk):\n",
    "    for k, v in weak_dict.items():\n",
    "        if v == wk:\n",
    "            return k\n",
    "        \n",
    "tmp_typ = [x for x in data[1:] if x[cols['type']] == 'NaN']\n",
    "nan_idx = [data.index(x) for x in tmp_typ]\n",
    "for i in nan_idx:\n",
    "    tp = wk2tp(data[i][cols['weakness']])\n",
    "    data[idx][cols['type']] = tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12355ae-a1df-4ce2-8fcf-03b2a3f403db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q1.3\n",
    "three = ['atk', 'def', 'hp']\n",
    "average_above_40 = {}\n",
    "average_40 = {}\n",
    "\n",
    "for tri in three:\n",
    "    tmp = [x for x in data[1:] if x[cols[tri]] != 'NaN']\n",
    "    tmp_hi = [x for x in tmp if float(x[cols['level']]) > 40]\n",
    "    tmp_hi = [float(x[cols[tri]]) for x in tmp_hi]\n",
    "    average_above_40[tri] = round(sum(tmp_hi) / len(tmp_hi),1)\n",
    "    \n",
    "    tmp_lo = [x for x in tmp if float(x[cols['level']]) <= 40]\n",
    "    tmp_lo = [float(x[cols[tri]]) for x in tmp_lo]\n",
    "    average_40[tri] = round(sum(tmp_lo) / len(tmp_lo),1)\n",
    "\n",
    "for trt in three:\n",
    "    tmp = [x for x in data[1:] if x[cols[tri]] == 'NaN']\n",
    "    tmp_idx = [data.index(x) for x in tmp]\n",
    "    for idx in tmp_idx:\n",
    "        data[idx][cols[tri]] = average_above_40[tri] if float(data[idx][cols['level']]) > 40 else average_40[tri]\n",
    "\n",
    "with open('pokemonResult.csv','w') as file:\n",
    "    myWriter = csv.writer(file)\n",
    "    for line in data:\n",
    "        myWriter.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c05bd0-d569-44c2-90ac-34b734cdc4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.1.4\n",
    "from collections import defaultdict\n",
    "\n",
    "types = list(set([x[cols['type']] for x in data[1:]]))\n",
    "types.sort()\n",
    "\n",
    "personality_dict = defaultdict(list)\n",
    "\n",
    "for typ in types:\n",
    "    tmpList = [x for x in data[1:] if x[cols['type']] == typ]\n",
    "    for pkm in tmpList:\n",
    "        personality_dict[typ].append(pkm[cols['personality']])\n",
    "new_personality_dict = {}\n",
    "print(new_personality_dict)\n",
    "\n",
    "for k,v in personality_dict.items():\n",
    "    v.sort()\n",
    "    new_personality_dict[k] = v\n",
    "    \n",
    "# with open('pokemon4.txt', 'w') as f:\n",
    "#     f.write('Pokemon type to personality mapping:' + '\\n')\n",
    "#     for k, v in new_personality_dict.items():\n",
    "#         f.write(k +': '+ ', '.join(v) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31117cca-dfef-4409-85b4-4474857c3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.1.5\n",
    "tmpList = [x for x in data[1:] if float(x[cols['stage']]) == 3]\n",
    "# print(len(tmpList))\n",
    "tmpList = [float(x[cols['hp']]) for x in tmpList]\n",
    "with open('pokemon5.txt', 'w') as f:\n",
    "    f.write('Average hit point for Pokemons of stage 3.0 = {}'.format(round(sum(tmpList) / len(tmpList))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1a92b-dbbb-4e46-94a8-d4ec9a8776f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "with open('covidTrain.csv','r') as myFile:\n",
    "    lines = csv.reader(myFile)\n",
    "    for line in lines:\n",
    "        data.append(line)\n",
    "names = data[0]\n",
    "\n",
    "cols = {}\n",
    "for i, col in enumerate(data[0]):\n",
    "    cols[col] = i\n",
    "# names = ['ID', 'age', 'sex', 'city', 'province', 'country', 'latitude',\n",
    "#          'longitude', 'date_onset_symptoms', 'date_admission_hospital',\n",
    "#          'date_confirmation', 'symptoms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13fd190-4337-4fe6-81d4-ccb66fce73bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.2.1\n",
    "tmpList = [x for x in data[1:] if '-' in x[cols['age']]]\n",
    "for line in tmpList:\n",
    "    age = line[cols['age']].strip().split('-')\n",
    "    num = [int(x) for x in age]\n",
    "    avg = round(sum(num)/2)\n",
    "    idx = data.index(line)\n",
    "    data[idx][cols['age']] = str(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f767bb9-58bb-4b82-9732-ff9b628b7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.2.2\n",
    "three = ['date_onset_symptoms', 'date_admission_hospital', 'date_confirmation']\n",
    "for date in three:\n",
    "    for i in range(1, len(data)):\n",
    "        dmy = data[i][cols[date]]\n",
    "        dmy = dmy.split('.')\n",
    "        mdy = [dmy[1], dmy[0], dmy[-1]]\n",
    "        mdy = '.'.join(mdy)\n",
    "        data[i][cols[date]] = mdy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e85ce-c091-4517-8785-750e0d066dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.2.3\n",
    "coordinates = ['latitude','longitude']\n",
    "provs = set([x[cols['province']] for x in data[1:]])\n",
    "for prov in provs:\n",
    "    for cod in coordinates:\n",
    "        tmpList = [x for x in data[1:] if x[cols['province']] == prov]\n",
    "        num = [x for x in tmpList if x[cols[cod]] != 'NaN']\n",
    "        nan = [x for x in tmpList if x[cols[ll]] == 'NaN']\n",
    "        val = [float(x[cols[cod]]) for x in num]\n",
    "        avg = round(sum(val)/len(val), 2)\n",
    "        for line in nan:\n",
    "            idx = data.index(line)\n",
    "            data[idx][cols[cod]] = str(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca50c8e-a374-42c4-886c-e188968dd717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.2.4\n",
    "provs = set([x[cols['province']] for x in data[1:]])\n",
    "for prov in provs:\n",
    "    tmpList = [x for x in data[1:] if x[cols['province']] == prov]\n",
    "    number = [x for x in tmpList if x[cols['city']] != 'NaN']\n",
    "    nan = [x for x in tmpList if x[cols['city']] == 'NaN']\n",
    "    cityList = set([x[cols['city']] for x in number])\n",
    "    city_num = []\n",
    "    \n",
    "    for citi in cityList:\n",
    "        num = len([x for x in good if x[cols['city']] == citi])\n",
    "        city_num.append((citi, num))\n",
    "    city_num.sort(key=lambda x: (-x[-1], x[0]))\n",
    "    \n",
    "    for line in nan:\n",
    "        idx = data.index(line)\n",
    "        data[idx][cols['city']] = city_num[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb112183-7a37-4ac4-b0af-58ffeb44be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.2.5\n",
    "provs = set([x[cols['province']] for x in data[1:]])\n",
    "for prov in provs:\n",
    "    tmpList = [x for x in data[1:] if x[cols['province']] == prov]\n",
    "    number = [x for x in tmpList if x[cols['symptoms']] != 'NaN']\n",
    "    symps = [x[cols['symptoms']] for x in number]\n",
    "    symps = ';'.join(symps)\n",
    "    symps = symps.split(';')\n",
    "    symps = [x.strip() for x in symps]\n",
    "    sympsList = []\n",
    "    for symp in set(symps):\n",
    "        num = symps.count(symp)\n",
    "        sympsList.append((symp, num))\n",
    "    sympsList.sort(key=lambda x: (-x[-1], x[0]))  \n",
    "\n",
    "    nan = [x for x in tmpList if x[cols['symptoms']] == 'NaN']\n",
    "    for line in nan:\n",
    "        idx = data.index(line)\n",
    "        data[idx][cols['symptoms']] = sympsList[0][0]\n",
    "        \n",
    "        \n",
    "# with open('covidResult.csv','w') as file:\n",
    "#     myWriter=csv.writer(file)\n",
    "#     for line in data:\n",
    "#         myWriter.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f23447b6-543b-4926-9084-004011c35c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('tfidf_docs.txt') as f:\n",
    "    files = f.read()\n",
    "\n",
    "files = files.split('\\n')\n",
    "files = [x for x in files if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ce616ab-bcb1-473c-8e82-28f10cb57262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q 3.1.1\n",
    "f = open(files[1])\n",
    "text = f.read()\n",
    "f.close()\n",
    "\n",
    "text = re.sub('\\n', ' ', text)\n",
    "text = re.sub('http.*? ', '', text)\n",
    "\n",
    "def rmvComma(line):\n",
    "    ptns = re.findall('\\d,\\d', line)\n",
    "    for ptn in ptns:\n",
    "        line = line.replace(ptn, ptn.replace(',', ''))\n",
    "    return line\n",
    "text = rmvComma(text)\n",
    "\n",
    "punctuation = r'-!#$%&\"()*+,./:;<=>?@[\\]^`{|}~\\''\n",
    "def rmvPunc(text):\n",
    "    text = re.sub(r'[{}]+'.format(punctuation),'',text)\n",
    "    return text.strip()\n",
    "text = rmvPunc(text)\n",
    "\n",
    "def rmvSpace(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "text = rmvSpace(text)\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "719d4a97-fc86-43e4-af98-b3ecaff53ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    stopwords = f.read()\n",
    "\n",
    "stopwords = stopwords.strip().split('\\n')\n",
    "\n",
    "text = text.split(' ')\n",
    "text = [x for x in text if not x in stopwords]\n",
    "text = ' '.join(text)\n",
    "\n",
    "# q 3.1.3\n",
    "text = text.split()\n",
    "def stemLem(word):\n",
    "    if word.endswith('ly'):\n",
    "        return word[:-2]\n",
    "    if word.endswith('ment'):\n",
    "        return word[:-4]\n",
    "    if word.endswith('ing'):\n",
    "        return word[:-3]\n",
    "    return word\n",
    "\n",
    "text = [stemLem(x) for x in text]\n",
    "text = ' '.join(text)\n",
    "\n",
    "# with open('preproc_' + files[1], 'w') as f:\n",
    "#     f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "619a7fc8-70b6-4dac-9737-f68044774f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contrary popular belief lorem ipsum simp random text roots piece classical latin literature 45 bc mak 2000 years old richard mcclintock latin professor hampdensydney college virginia looked one obscure latin words consectetur lorem ipsum passage go cites word classical literature discovered undoubtable source lorem ipsum comes sections de finibus bonorum et malorum extremes good evil cicero written 45 bc book treatise theory ethics popular renaissance first line lorem ipsum lorem ipsum dolor sit amet comes line section 32 edited nightbot_ggwp'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1af87d58-cd88-4999-ba6a-c53dbe8768f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q 3.2\n",
    "texts = []\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        text = f.read()\n",
    "        texts.append(text)\n",
    "\n",
    "def prePro(text):\n",
    "    file = files[texts.index(text)]\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('http.*? ', '', text)\n",
    "    text = rmvComma(text)\n",
    "    text = rmvPunc(text)\n",
    "    text = rmvSpace(text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.split(' ')\n",
    "    text = [x for x in text if not x in stopwords]\n",
    "    totol_num_of_words = len(text)\n",
    "    text = [stemLem(x) for x in text]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "#     with open('preproc_' + file, 'w') as f:\n",
    "#         f.write(text)\n",
    "\n",
    "    return text, totol_num_of_words\n",
    "\n",
    "tf_dict = {}\n",
    "terms = []\n",
    "for i in range(len(files)):\n",
    "    text = texts[i]\n",
    "    text, total_num_of_words = prePro(text)\n",
    "    text = text.split(' ')\n",
    "    total_num_of_words = len(text)\n",
    "\n",
    "    keys = set(text)\n",
    "    terms.extend(list(keys))\n",
    "    tf_dict[files[i]] = {}\n",
    "    \n",
    "    for key in keys:\n",
    "        num = text.count(key)\n",
    "        tf_dict[files[i]][key] = num / total_num_of_words\n",
    "\n",
    "terms = list(set(terms))\n",
    "idf_dict = {}\n",
    "total_num_of_docs = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eb0f6d20-7b9d-4292-b4ac-5bf3c473308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0])\n"
     ]
    }
   ],
   "source": [
    "for term in terms:\n",
    "    num = sum([term in x.keys() for x in tf_dict.values()])\n",
    "    if num:\n",
    "        idf = total_num_of_docs / num\n",
    "    else:\n",
    "        idf = 0\n",
    "    idf_dict[term] = idf\n",
    "print(idf_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c1e11432-b10f-40bc-8021-dcacbf13aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, add 1 to the IDF score so that the TF-IDF score is non-zero.\n",
    "# for k, v in idf_dict.items():\n",
    "#     idf_dict[k] = v + 1\n",
    "#     print(k, '->', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2ed4abc8-9a04-45d1-95b3-babd9f673d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('latin', 0.07), ('lorem', 0.06), ('ipsum', 0.06), ('45', 0.05), ('literature', 0.05), ('bc', 0.05), ('comes', 0.05), ('line', 0.05), ('classical', 0.05), ('popular', 0.05), ('source', 0.03), ('treatise', 0.03), ('section', 0.03), ('word', 0.03), ('richard', 0.03), ('mak', 0.03), ('de', 0.03), ('extremes', 0.03), ('contrary', 0.03), ('sections', 0.03), ('undoubtable', 0.03), ('good', 0.03), ('words', 0.03), ('piece', 0.03), ('theory', 0.03), ('evil', 0.03), ('professor', 0.03), ('dolor', 0.03), ('amet', 0.03), ('looked', 0.03), ('hampdensydney', 0.03), ('malorum', 0.03), ('first', 0.03), ('et', 0.03), ('random', 0.03), ('belief', 0.03), ('written', 0.03), ('renaissance', 0.03), ('cicero', 0.03), ('virginia', 0.03), ('roots', 0.03), ('old', 0.03), ('obscure', 0.03), ('go', 0.03), ('sit', 0.03), ('2000', 0.03), ('years', 0.03), ('consectetur', 0.03), ('32', 0.03), ('discovered', 0.03), ('college', 0.03), ('passage', 0.03), ('nightbot_ggwp', 0.03), ('ethics', 0.03), ('mcclintock', 0.03), ('one', 0.03), ('bonorum', 0.03), ('cites', 0.03), ('finibus', 0.03), ('edited', 0.01), ('book', 0.01), ('simp', 0.01), ('text', 0.01)]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_dict = {}\n",
    "for file, tfs in tf_dict.items():\n",
    "    tf_idf_dict[file] = {}\n",
    "    for k, v in tfs.items():\n",
    "        tf_idf = round(v * idf_dict[k], 2)\n",
    "        tf_idf_dict[file][k] = tf_idf\n",
    "        \n",
    "for file, tf_idf in tf_idf_dict.items():\n",
    "    tmp = sorted(tf_idf.items(), key=lambda item:item[1], reverse=True)\n",
    "print(tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
